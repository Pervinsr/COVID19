# -*- coding: utf-8 -*-
"""CoronaVirus Pandemic EDA and Forecasting

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rYdgvz0Y0mEn_9iuug905Duqp6fYAxwc

# CoronaVirus Pandemic: EDA for core pandemic metrics

This notebook is focused on

- exhaustive EDA of COVID-19 pandemic metrics
- correlation of COVID-19 pandemic metrics (confirmed, deaths, recovered) with weather parameters (to check the hypothesis of COVID-19 to be weather-sensitive, like influenza viruses)
- correlation of COVID-19 pandemic metrics (confirmed, deaths, recovered) with macroeconomic params and wealness/cultural attitude in different countries (to seee if the significant impact of culture/national economies on the coronavirus spread can be confirmed)
- Calculating the forecasts CoronaVirus spread for the next 7 calendar days
"""

!pip install pdpipe

import pandas as pd
import pdpipe as pdp
import numpy as np

from google.colab import drive
from google.colab import auth

import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt

from sklearn import preprocessing
import time
from datetime import datetime

"""## Pre-processing

We will load public open data about Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE (https://systems.jhu.edu/research/public-health/ncov/), as per their online repository at https://github.com/CSSEGISandData/COVID-19 (updated daily)
"""

confirmed_ts_df = pd.read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv")
deaths_ts_df = pd.read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv")
recovered_ts_df = pd.read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv")

"""Now we will melt and transform the canonic time series data into a long-form dataframes"""

confirmed_ts_melted_df = confirmed_ts_df.melt(id_vars=['Province/State', 'Country/Region', 'Lat', 'Long',],
                     var_name='covid_date', value_name='confirmed').copy()

deaths_ts_melted_df = deaths_ts_df.melt(id_vars=['Province/State', 'Country/Region', 'Lat', 'Long',],
                     var_name='covid_date', value_name='deaths').copy()

recovered_ts_melted_df = recovered_ts_df.melt(id_vars=['Province/State', 'Country/Region', 'Lat', 'Long',],
                     var_name='covid_date', value_name='recovered').copy()

"""Now we will further transform the data as well as put them into the single dataframe"""

pipeline = pdp.PdPipeline([
    pdp.ApplyByCols(['covid_date'], pd.to_datetime),
    pdp.ColRename({'Country/Region': 'country_id', 'Province/State': 'state', 'Lat': 'lat', 'Long': 'long'})
])

confirmed_ts_fcg_df = pipeline.apply(confirmed_ts_melted_df).sort_values(by=['country_id', 'covid_date'])
deaths_ts_fcg_df = pipeline.apply(deaths_ts_melted_df).sort_values(by=['country_id', 'covid_date'])
recovered_ts_fcg_df = pipeline.apply(recovered_ts_melted_df).sort_values(by=['country_id', 'covid_date'])

covid_df = pd.merge(
    pd.merge(confirmed_ts_fcg_df, deaths_ts_fcg_df, on=['state', 'country_id', 'lat', 'long','covid_date']),
    recovered_ts_fcg_df, on=['state', 'country_id', 'lat', 'long','covid_date'])

covid_df.head()

"""The date range covered by the current dataset is as follows"""

covid_start_date = covid_df["covid_date"].dt.date.min()
covid_end_date = covid_df["covid_date"].dt.date.max()
print("Start Date:", covid_start_date)
print("End Date:", covid_end_date)

"""## Loading Additional Datasets

Now we are going to load the log of COVID-19 national containment measures
"""

# This will mount the drive to this notebook
drive.mount('/content/drive')

base_dataset_folder = '/content/drive/My Drive/CoronaDatasets'

# Load the log of COVID-19 Containment measures
covid_containment_path = base_dataset_folder + '/covid19-national-responses-dataset/COVID 19 Containment measures data.csv'

covid_containment_master_df = pd.read_csv(covid_containment_path)

pipeline = pdp.PdPipeline([
    pdp.ColRename({
        'ID': 'measure_id',
        'Applies To': 'applied_to',
        'Country': 'country_id', 	
        'Date Start': 'start_date',
        'Date end intended': 'end_date',
        'Description of measure implemented': 'desc',
        'Exceptions': 'exceptions',	
        'Implementing City': 'city',
        'Implementing State/Province': 'state',	
        'Keywords': 'keywords',	
        'Quantity': 'qnty',	
        'Source': 'source',	
        'Target city': 'target_city',	
        'Target country': 'target_country',
        'Target region': 'target_region',
        'Target state': 'target_state'
        }),
    pdp.ApplyByCols(['start_date'], pd.to_datetime),
    pdp.ColDrop(['measure_id','applied_to', 'end_date', 'qnty', 
                 'exceptions', 'qnty', 'target_city', 'target_country',
                 'target_region', 'target_state', 'city', 'state'])
])

covid_containment_df = pipeline.apply(covid_containment_master_df)

covid_containment_df.head(5)

"""## Global Tendencies: World (except China)"""

countries_without_china = covid_df['country_id'].unique().tolist()

countries_without_china.remove('China')

pipeline = pdp.PdPipeline([
    pdp.ValKeep(countries_without_china, columns=['country_id']),
])

total_noChina = pipeline.apply(covid_df)

confirmed_total_date_noChina = total_noChina.groupby(['covid_date']).agg({'confirmed':['sum']})
fatalities_total_date_noChina = total_noChina.groupby(['covid_date']).agg({'deaths':['sum']})
total_date_noChina = confirmed_total_date_noChina.join(fatalities_total_date_noChina)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))
total_date_noChina.plot(ax=ax1)
ax1.set_title("Global confirmed cases excluding China", size=13)
ax1.set_ylabel("Number of cases", size=13)
ax1.set_xlabel("Date", size=13)
fatalities_total_date_noChina.plot(ax=ax2, color='orange')
ax2.set_title("Global deceased cases excluding China", size=13)
ax2.set_ylabel("Number of cases", size=13)
ax2.set_xlabel("Date", size=13)

"""## Global Tendencies: China"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['China'], columns=['country_id']),
])

total_China = pipeline.apply(covid_df)

confirmed_total_date_China = total_China.groupby(['covid_date']).agg({'confirmed':['sum']})
fatalities_total_date_China = total_noChina.groupby(['covid_date']).agg({'deaths':['sum']})
total_date_China = confirmed_total_date_China.join(fatalities_total_date_China)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))
total_date_China.plot(ax=ax1)
ax1.set_title("China confirmed cases", size=13)
ax1.set_ylabel("Number of cases", size=13)
ax1.set_xlabel("Date", size=13)
fatalities_total_date_China.plot(ax=ax2, color='orange')
ax2.set_title("China deceased cases", size=13)
ax2.set_ylabel("Number of cases", size=13)
ax2.set_xlabel("Date", size=13)

"""## Business-Related Tendencies: Operational Markets

### Overview
"""

fcg_countries = {'Argentina': 'AR',
                 'Chile': 'CL',
                 'Colombia': 'CO',
                 'Mexico': 'MX',
                 'Nigeria': 'NG',
                 'Kenya': 'KE',
                 'Ghana': 'GH',
                 'Poland': 'PL', 
                 'United Arab Emirates': 'AE',
                 'India': 'IN',
                 'Indonesia': 'ID',
                 'Pakistan': 'PK'
                 }

pipeline = pdp.PdPipeline([
    pdp.ValKeep(fcg_countries.keys(), columns=['country_id']),
    pdp.MapColVals(['country_id'], fcg_countries)

])

covid_fcg_df = pipeline.apply(covid_df)

covid_containment_fcg_df = pipeline.apply(covid_containment_df)

covid_fcg_df.head(5)

fig = px.line(covid_fcg_df, x='covid_date', y='confirmed', color='country_id', title='Confirmed Cases of COVID-19 Evolution in the Markets')
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Confirmed Cases')
fig.show()

fig = px.line(covid_fcg_df, x='covid_date', y='deaths', color='country_id', title='Death Cases of COVID-19 Evolution in the Markets')
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Fatal Cases (Deaths)')
fig.show()

fig = px.line(covid_fcg_df, x='covid_date', y='recovered', color='country_id', title='Recovered Cases of COVID-19 Evolution in the Markets')
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Recovery Cases')
fig.show()

px.bar(covid_fcg_df[['country_id', 'confirmed', 'deaths', 'recovered']].groupby(by=['country_id'], as_index=False).max().sort_values(by='confirmed'),
       x='country_id', y='confirmed', title='Total Current COVID-19 Confirmed Cases in the Markets', )

"""### Trends in Pakistan"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['PK'], columns=['country_id']),
])

covid_containment_pk_df = pipeline.apply(covid_containment_fcg_df)

temp = pipeline.apply(covid_fcg_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: Pakistan', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""### Log of Containment Measures in Pakistan"""

if covid_containment_pk_df.shape[0] > 0:
  display(covid_containment_pk_df)
else:
  print("No containment measures  documented by http://epidemicforecasting.org/containment")

"""### Trends in Poland"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['PL'], columns=['country_id']),
])

covid_containment_pl_df = pipeline.apply(covid_containment_fcg_df)

temp = pipeline.apply(covid_fcg_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: Poland', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""### Log of Containment Measures in Poland"""

if covid_containment_pl_df.shape[0] > 0:
  display(covid_containment_pl_df)
else:
  print("No containment measures  documented by http://epidemicforecasting.org/containment")

"""### Trends in Chile"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['CL'], columns=['country_id']),
])

covid_containment_cl_df = pipeline.apply(covid_containment_fcg_df)

temp = pipeline.apply(covid_fcg_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: Chile', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""### Log of Containment Measures in Chile"""

if covid_containment_cl_df.shape[0] > 0:
  display(covid_containment_cl_df)
else:
  print("No containment measures documented by http://epidemicforecasting.org/containment")

"""### Trends in Indonesia"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['ID'], columns=['country_id']),
])

covid_containment_id_df = pipeline.apply(covid_containment_fcg_df)

temp = pipeline.apply(covid_fcg_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: Indonesia', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""**Note**: Indonesia is currently in the sad list of top 10 countries with the highest COVID-19-driven mortality ratio across the globe

### Log of Containment Measures in Indonesia
"""

if covid_containment_id_df.shape[0] > 0:
  display(covid_containment_id_df)
else:
  print("No containment measures documented by http://epidemicforecasting.org/containment")

"""### Trends in India"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['IN'], columns=['country_id']),
])

covid_containment_in_df = pipeline.apply(covid_containment_fcg_df)

temp = pipeline.apply(covid_fcg_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: India', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""### Log of Containment Activities in India"""

if covid_containment_in_df.shape[0] > 0:
  display(covid_containment_in_df)
else:
  print("No containment measures documented by http://epidemicforecasting.org/containment")

"""## Business-Related Tendencies: Key Software Development Locations"""

sd_countries = {'Germany': 'DE',
                 'Sweden': 'SE',
                 'Ukraine': 'UA'
                 }

pipeline = pdp.PdPipeline([
    pdp.ValKeep(sd_countries.keys(), columns=['country_id']),
    pdp.MapColVals(['country_id'], sd_countries)

])

covid_sd_df = pipeline.apply(covid_df)

covid_containment_sd_df = pipeline.apply(covid_containment_df)

"""### Trends in Germany"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['DE'], columns=['country_id']),
])

covid_containment_de_df = pipeline.apply(covid_containment_sd_df)

temp = pipeline.apply(covid_sd_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: Germany', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""### Containment measures in Germany"""

if covid_containment_de_df.shape[0] > 0:
  display(covid_containment_de_df)
else:
  print("No containment measures documented by http://epidemicforecasting.org/containment")

"""### Trends in Sweden"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['SE'], columns=['country_id']),
])

covid_containment_se_df = pipeline.apply(covid_containment_sd_df)

temp = pipeline.apply(covid_sd_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: Sweden', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""### Containment Measures in Sweden"""

if covid_containment_se_df.shape[0] > 0:
  display(covid_containment_se_df)
else:
  print("No containment measures documented by http://epidemicforecasting.org/containment")

"""### Trends in Ukraine"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['UA'], columns=['country_id']),
])

temp = pipeline.apply(covid_sd_df)

temp = temp.groupby('covid_date')['recovered', 'deaths', 'confirmed'].sum().reset_index()
temp = temp.melt(id_vars="covid_date", value_vars=['recovered', 'deaths', 'confirmed'],
                 var_name='case', value_name='count')

fig = px.area(temp, x="covid_date", y="count", color='case',
             title='Cases over time: Ukraine', color_discrete_sequence = ['cyan', 'red', 'orange'])
fig.update_xaxes(title_text='Date')
fig.update_yaxes(title_text='Cases')
fig.show()

"""## Global insights: Mortality and Recovery Rates¶
Mortality and Recovery rates as of the last date in the time series are worth seeing as well. It might have an insight for sure.

### Mortality Rates
"""

cleaned_latest = covid_df[covid_df['covid_date'] == max(covid_df['covid_date'])]

flg = cleaned_latest.groupby('country_id')['confirmed', 'deaths', 'recovered'].sum().reset_index()

flg['mortalityRate'] = round((flg['deaths']/flg['confirmed'])*100, 2)
temp = flg[flg['confirmed']>100]
temp = temp.sort_values('mortalityRate', ascending=False)

fig = px.bar(temp.sort_values(by="mortalityRate", ascending=False)[:10][::-1],
             x = 'mortalityRate', y = 'country_id', 
             title='Deaths per 100 Confirmed Cases', text='mortalityRate', height=800, orientation='h',
             color_discrete_sequence=['darkred']
            )
fig.show()

"""* Algeria is at the top of the list, trending up.
* Italy is still one most brutal of them all (however, Italy trending down by this metric, which is a good news).
* Belgium is trending up, sign of worry for them.
* Netherlands, UK, Belgium, France, and Spain being the most notable ones too.
* Indonesia appeared in the top 10 list again as of Apr 13, 2010 (replacing Bangladesh).
* Bangladesh got off the top 10 list (a good sign for them)
* Sweden entered the top ten list

**Note**: some other countries with poor medical system and unsufficient COVID-19 test system coverage may not display truly genuine charts in this view

### Countries with Lowest Mortality Rates
"""

temp = flg[flg['confirmed']>100]
temp = temp.sort_values('mortalityRate', ascending=True)[['country_id', 'confirmed','deaths']][:16]
temp.sort_values('confirmed', ascending=False)[['country_id', 'confirmed','deaths']][:20].style.background_gradient(cmap='Blues')

"""### Recovery Rates"""

flg['recoveryRate'] = round((flg['recovered']/flg['confirmed'])*100, 2)
temp = flg[flg['confirmed']>100]
temp = temp.sort_values('recoveryRate', ascending=False)

fig = px.bar(temp.sort_values(by="recoveryRate", ascending=False)[:10][::-1],
             x = 'recoveryRate', y = 'country_id', 
             title='Recoveries per 100 Confirmed Cases', text='recoveryRate', height=800, orientation='h',
             color_discrete_sequence=['#2ca02c']
            )
fig.show()

"""* China can recover an estimated 93-94 out of every 100 that get affected. That's great and the numbers seem to increase for them every day.
* One of the cruise ships (Diamand Princess) is also on the radar
* Bahrain, South Korea, and Bruney as we can see are doing really well in terms of recovering.
* Switzerland entered the top 10 list in terms of recoveries (good sign for them)
* Italy, Spain, US, Netherlands, France, UK and Germany... Where are they? Is that why they are in trouble now?

### Countries with the Worst Recovery Rates
"""

temp = flg[flg['confirmed']>100]
temp = temp.sort_values('recoveryRate', ascending=True)[['country_id', 'confirmed','recovered']][:20]
temp.sort_values('confirmed', ascending=False)[['country_id', 'confirmed','recovered']][:20].style.background_gradient(cmap='Oranges')

"""## History: How did it happen?

### Worldwide History: Confirmed Cases
"""

formated_gdf = covid_df.groupby(['covid_date', 'country_id'])['confirmed', 'deaths'].max()
formated_gdf = formated_gdf.reset_index()
formated_gdf['covid_date'] = pd.to_datetime(formated_gdf['covid_date'])
formated_gdf['covid_date'] = formated_gdf['covid_date'].dt.strftime('%m/%d/%Y')
formated_gdf['size'] = formated_gdf['confirmed'].pow(0.3)

fig = px.scatter_geo(formated_gdf, locations="country_id", locationmode='country names', 
                     color="confirmed", size='size', hover_name="country_id", 
                     range_color= [0, 1500], 
                     projection="natural earth", animation_frame="covid_date", 
                     title='COVID-19: Spread Over Time', color_continuous_scale="portland")
fig.show()

"""* At the beginning (since the end of Jan 2020 as of the earliest data available) the disease seems to be only around China and its neighboring countries.

* However it quickly spread off to Europe, Autralia and even the US which is very interesting.

* Things seemed to be in fairly good mode for European locations even in mid February 2020.

* West Asia, especially Iran and Iraq, started to catch fire at the end of February 2020, along with Italy showing signs of the dread to come. South Korea and China were peaking at that moment.

* By March 5, 2020 the situation in Europe started to go out of the control. They could've have locked down right at that moment.

* The disease has taken away Africa and Americas too by early March, with alarm bells ringing loudly for the US with just over 500 cases.

* According to the data so far, USA, UK, Spain, Italy, Germany, France and the UK are in deep trouble. Next few days are crucial for how the disease develops around the world.

### Worldwide History: Deaths over Time
"""

formated_gdf = covid_df.groupby(['covid_date', 'country_id'])['confirmed', 'deaths'].max()
formated_gdf = formated_gdf.reset_index()
formated_gdf['covid_date'] = pd.to_datetime(formated_gdf['covid_date'])
formated_gdf['covid_date'] = formated_gdf['covid_date'].dt.strftime('%m/%d/%Y')
formated_gdf['size'] = formated_gdf['deaths'].pow(0.3)

fig = px.scatter_geo(formated_gdf, locations="country_id", locationmode='country names', 
                     color="deaths", size='size', hover_name="country_id", 
                     range_color= [0, 100], 
                     projection="natural earth", animation_frame="covid_date", 
                     title='COVID-19: Deaths Over Time', color_continuous_scale="peach")

fig.show()

"""The most interesting insights here are
  

*   China's relative steadiness since March
*   Europe's severe spreads.

### Europe's History: Confirmed Cases
"""

formated_gdf = covid_df.groupby(['covid_date', 'country_id'])['confirmed', 'deaths', 'recovered'].max()
formated_gdf = formated_gdf.reset_index()
formated_gdf['covid_date'] = pd.to_datetime(formated_gdf['covid_date'])
formated_gdf['covid_date'] = formated_gdf['covid_date'].dt.strftime('%m/%d/%Y')
formated_gdf['size'] = formated_gdf['confirmed'].pow(0.3) * 5

fig = px.scatter_geo(formated_gdf, locations="country_id", locationmode='country names', 
                     color="confirmed", size='size', hover_name="country_id", 
                     range_color= [0, 5000], 
                     projection="natural earth", animation_frame="covid_date", scope="europe",
                     title='COVID-19: Spread Over Time in EUROPE', color_continuous_scale="portland", height=800)
fig.show()

"""* According to the official dataset, it looks to be France who are first affected by the virus. 
* However, the data set lacks the hidden story about the remarkable football match in Bergamo that caused much stronger spread in Italy and Spain.
* Nothing bad has happened in Europe as long as mid February. End of February, Italy inflates considerably.
* Entire West Europe overshadowed by the virus by the mid of March 2020.

### Europe's History: Fatal Cases (Deaths)
"""

formated_gdf['covid_date'] = pd.to_datetime(formated_gdf['covid_date'])
formated_gdf['covid_date'] = formated_gdf['covid_date'].dt.strftime('%m/%d/%Y')
formated_gdf['size'] = formated_gdf['deaths'].pow(0.3)

fig = px.scatter_geo(formated_gdf, locations="country_id", locationmode='country names', 
                     color="deaths", size='size', hover_name="country_id", 
                     range_color= [0, 500], 
                     projection="natural earth", animation_frame="covid_date", scope="europe",
                     title='COVID-19: Deaths Over Time in EUROPE', color_continuous_scale="peach", height=800)
fig.show()

"""* At some point in Feb 2020, most of the EU countries seemed to have no active cases (except Italy).
* However, then we observe severe spread in Italy and other EU locations 
* The possible reason was the continual active spread in Italy under the lack of locking down public events and travel ban there

### Europe's History: Recovery Cases
"""

formated_gdf['covid_date'] = pd.to_datetime(formated_gdf['covid_date'])
formated_gdf['covid_date'] = formated_gdf['covid_date'].dt.strftime('%m/%d/%Y')
formated_gdf['size'] = formated_gdf['recovered'].pow(0.3) * 3.5

fig = px.scatter_geo(formated_gdf, locations="country_id", locationmode='country names', 
                     color="recovered", size='size', hover_name="country_id", 
                     range_color= [0, 100], 
                     projection="natural earth", animation_frame="covid_date", scope="europe",
                     title='COVID-19: Recovered Cases Over Time in EUROPE', color_continuous_scale="greens", height=800)
fig.show()

"""# Feature Enrichment

## Basic clean-up, dates, lag, and trend features
"""

# Basic clean-up and dates features

le = preprocessing.LabelEncoder()

covid_df_corr = covid_df.copy()
covid_df_corr['day_num'] = le.fit_transform(covid_df_corr.covid_date)
covid_df_corr['day'] = covid_df_corr['covid_date'].dt.day
covid_df_corr['month'] = covid_df_corr['covid_date'].dt.month
covid_df_corr['year'] = covid_df_corr['covid_date'].dt.year

# Fill null values given that we merged train-test datasets
covid_df_corr['state'].fillna("None", inplace=True)
covid_df_corr['confirmed'].fillna(0, inplace=True)
covid_df_corr['deaths'].fillna(0, inplace=True)
covid_df_corr['recovered'].fillna(0, inplace=True)

# lag and trend calculation
def calculate_trend(df, lag_list, column):
    for lag in lag_list:
        trend_column_lag = "Trend_" + column + "_" + str(lag)
        df[trend_column_lag] = (df[column]-df[column].shift(lag, fill_value=-999))/df[column].shift(lag, fill_value=0)
    return df
  
def calculate_lag(df, lag_list, column):
    for lag in lag_list:
        column_lag = "Lag_" + column + "_" + str(lag)
        df[column_lag] = df[column].shift(lag, fill_value=0)
    return df

# spread ratio calculation
def calculate_spread_ratio(df, lag_list, column):
    for lag in lag_list:
        trend_column_lag = "Spread_" + column + "_" + str(lag)
        df[trend_column_lag] = df[column]/df[column].shift(lag, fill_value=0)
    return df

ts = time.time()
covid_df_corr = calculate_lag(covid_df_corr, range(1,7), 'confirmed')
covid_df_corr = calculate_lag(covid_df_corr, range(1,7), 'deaths')
covid_df_corr = calculate_lag(covid_df_corr, range(1,7), 'recovered')
covid_df_corr = calculate_trend(covid_df_corr, range(1,7), 'confirmed')
covid_df_corr = calculate_trend(covid_df_corr, range(1,7), 'deaths')
covid_df_corr = calculate_trend(covid_df_corr, range(1,7), 'recovered')
covid_df_corr = calculate_spread_ratio(covid_df_corr, range(1,7), 'confirmed')
covid_df_corr = calculate_spread_ratio(covid_df_corr, range(1,7), 'deaths')
covid_df_corr = calculate_spread_ratio(covid_df_corr, range(1,7), 'recovered')
covid_df_corr.replace([np.inf, -np.inf], 0, inplace=True)
covid_df_corr.fillna(0, inplace=True)
print("Time spent: ", time.time()-ts)

"""## Adding Recovery and Deaths Rates"""

covid_df_corr['recovery_rate'] = round((covid_df_corr['recovered']/covid_df_corr['confirmed'])*100, 2)
covid_df_corr['mortality_rate'] = round((covid_df_corr['deaths']/covid_df_corr['confirmed'])*100, 2)

"""## Adding Exponential Moving Average"""

# Ref.: https://www.datacamp.com/community/tutorials/moving-averages-in-pandas
covid_df_corr['confirmed_ema'] = covid_df_corr['confirmed'].ewm(span=40,adjust=False).mean()
covid_df_corr['deaths_ema'] = covid_df_corr['deaths'].ewm(span=40,adjust=False).mean()
covid_df_corr['recovered_ema'] = covid_df_corr['recovered'].ewm(span=40,adjust=False).mean()

display(covid_df_corr.columns.values.tolist())

display(covid_df_corr.head(10))

# now saving the enriched covid-19 metrics dataframe as a local file
local_file_path = base_dataset_folder + "/covid_enriched_" + covid_end_date.strftime("%b_%d_%Y") + ".csv"

covid_df_corr.to_csv(local_file_path, index=False)

print("Output completed: ", local_file_path)

"""## Basic Trend, Spread, Lag, and EMA visualizations

### Trends of Pandemy in Germany
"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['Germany'], columns=['country_id']),
])

temp = pipeline.apply(covid_df_corr)

"""Confirmed case lags as well as EMA are illustrated below"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed_ema'],
                    mode='lines+markers',
                    name='confirmed_ema'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_1'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_2'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_3'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_4'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_5'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_6'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed'],
                    mode='lines+markers', 
                    name='confirmed'))
# Edit the layout
fig.update_layout(title='Lags and EMA in Germany: Confirmed cases',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

"""As we can see, the charts for every Lag feature (*Lag_confirmed_1 - Lag_confirmed_6* are almost identical, and therefore we will be using *Lag_confirmed_1* only, in the visualizations for other countries below)

Trends and Spread Rates (vs. 6 days ago) are illustrated below
"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Trend_confirmed_6'],
                    mode='lines+markers', 
                    name='Trend_confirmed_6'))

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Spread_confirmed_6'],
                    mode='lines+markers', 
                    name='Spread_confirmed_6'))

# Edit the layout
fig.update_layout(title='Trends in Germany: Confirmed cases, Trend and Spread vs. 6 days ago',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

"""### Trends of Pandemy in South Korea"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['Korea, South'], columns=['country_id']),
])

temp = pipeline.apply(covid_df_corr)

"""Confirmed case lags as well as EMA are illustrated below"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed_ema'],
                    mode='lines+markers',
                    name='confirmed_ema'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_1'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed'],
                    mode='lines+markers', 
                    name='confirmed'))
# Edit the layout
fig.update_layout(title='Lags and EMA in South Korea: Confirmed cases',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

"""Trends and Spread Rates (vs. 6 days ago) are illustrated below"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Trend_confirmed_6'],
                    mode='lines+markers', 
                    name='Trend_confirmed_6'))

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Spread_confirmed_6'],
                    mode='lines+markers', 
                    name='Spread_confirmed_6'))

# Edit the layout
fig.update_layout(title='Trends in South Korea: Confirmed cases, Trend and Spread vs. 6 days ago',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

"""### Trends of Pandemy in Singapore"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['Singapore'], columns=['country_id']),
])

temp = pipeline.apply(covid_df_corr)

"""Confirmed case lags as well as EMA are illustrated below"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed_ema'],
                    mode='lines+markers',
                    name='confirmed_ema'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_1'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed'],
                    mode='lines+markers', 
                    name='confirmed'))
# Edit the layout
fig.update_layout(title='Lags and EMA in Singapore: Confirmed cases',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

"""Trends and Spread Rates (vs. 6 days ago) are illustrated below"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Trend_confirmed_6'],
                    mode='lines+markers', 
                    name='Trend_confirmed_6'))

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Spread_confirmed_6'],
                    mode='lines+markers', 
                    name='Spread_confirmed_6'))

# Edit the layout
fig.update_layout(title='Trends in Singapore: Confirmed cases, Trend and Spread vs. 6 days ago',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

"""### Trends of Pandemy in Ukraine"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep(['Ukraine'], columns=['country_id']),
])

temp = pipeline.apply(covid_df_corr)

"""Confirmed case lags as well as EMA are illustrated below"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed_ema'],
                    mode='lines+markers',
                    name='confirmed_ema'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Lag_confirmed_1'],
                    mode='lines+markers',
                    name='Lag_confirmed_1'))
fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['confirmed'],
                    mode='lines+markers', 
                    name='confirmed'))
# Edit the layout
fig.update_layout(title='Lags and EMA in Ukraine: Confirmed cases',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

"""Trends and Spread Rates (vs. 6 days ago) are illustrated below"""

# History of CoronaVirus spread: Create traces
fig = go.Figure()

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Trend_confirmed_6'],
                    mode='lines+markers', 
                    name='Trend_confirmed_6'))

fig.add_trace(go.Scatter(x=temp['covid_date'], y=temp['Spread_confirmed_6'],
                    mode='lines+markers', 
                    name='Spread_confirmed_6'))

# Edit the layout
fig.update_layout(title='Trends in Ukraine: Confirmed cases, Trend and Spread vs. 6 days ago',
                   xaxis_title='Date',
                   yaxis_title='Number of cases')
fig.show()

display(covid_df_corr.columns)

"""## Correlation of the basic pandemic features"""

# Correlation of COVID-19 pandemic indicators
corr_transform = pdp.PdPipeline([
    pdp.ColDrop ({'state',	'country_id',	'lat',	'long',	'covid_date', 'year',
                  'day', 'month',
      'Spread_confirmed_1', 'Spread_confirmed_2', 'Spread_confirmed_3',
      'Spread_confirmed_4', 'Spread_confirmed_5',  'Spread_confirmed_6', 
      'Spread_recovered_1', 'Spread_recovered_2', 'Spread_recovered_3',
      'Spread_recovered_4', 'Spread_recovered_5',  'Spread_recovered_6', 
      'Spread_deaths_1', 'Spread_deaths_2', 'Spread_deaths_3',
      'Spread_deaths_4', 'Spread_deaths_5',  'Spread_deaths_6' 
    })
])


data = corr_transform.apply(covid_df_corr)
corr = data.corr()

ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation of Basic COVID-19 Metrics')
ax

display(corr)

"""## More Detailed Outlook on Correlations with Confirmed"""

def heatmap_numeric_w_dependent_variable(df, dependent_variable):
    '''
    Takes df, a dependant variable as str
    Returns a heatmap of all independent variables' correlations with dependent variable 
    '''
    plt.figure(figsize=(8, 10))
    g = sns.heatmap(df.corr()[[dependent_variable]].sort_values(by=dependent_variable), 
                    annot=True, 
                    cmap='coolwarm', 
                    vmin=-1,
                    vmax=1) 
    return g

# check correlations with Confirmed
corr_transform = pdp.PdPipeline([
    pdp.ColDrop ({'state',	'lat',	'long', 'year',
                  'day', 'month',
      'Spread_confirmed_1', 'Spread_confirmed_2', 'Spread_confirmed_3',
      'Spread_confirmed_4', 'Spread_confirmed_5',  'Spread_confirmed_6', 
      'Spread_recovered_1', 'Spread_recovered_2', 'Spread_recovered_3',
      'Spread_recovered_4', 'Spread_recovered_5',  'Spread_recovered_6', 
      'Spread_deaths_1', 'Spread_deaths_2', 'Spread_deaths_3',
      'Spread_deaths_4', 'Spread_deaths_5',  'Spread_deaths_6' 
    })
])
covid_df_corr = corr_transform.apply(covid_df_corr)
heatmap_numeric_w_dependent_variable(covid_df_corr, 'confirmed')

"""We find that the following feature variables show strong and medium correlations with _confirmed_ cases

- deaths
- Lag_confirmed_1
- Lag_confirmed_2
- Lag_confirmed_3
- Lag_confirmed_4
- Lag_deaths_1
- Lag_confirmed_5
- Lag_deaths_2
- Lag_confirmed_6
- recovered
- confirmed_ema
- Lag_deaths_3
- Lag_deaths_4
- Lag_deaths_5
- deaths_ema
- Lag_recovered_1 (*)
- Lag_deaths_6 (*)

## More Detailed Outlook on Correlations with Deaths
"""

# check correlations with Deaths
heatmap_numeric_w_dependent_variable(covid_df_corr, 'deaths')

"""We find that the following feature variables show strong and medium correlations with _fatal cases (deaths)_

- confirmed
- Lag_deaths_1
- Lag_confirmed_1
- Lag_confirmed_2
- Lag_deaths_4
- deaths_ema
- Lag_confirmed_4
- Lag_confirmed_5 (*)
- Lag_confirmed_6 (*)

## More Detailed Outlook on Correlations with Recovered
"""

# check correlations with Recovered
heatmap_numeric_w_dependent_variable(covid_df_corr, 'recovered')

"""We find that the following feature variables display the strong and mediumm correlation with the number of _recovered_ cases

- Trend_deaths_3
- confirmed
- deaths

## More Detailed Outlook on Correlations with confirmed_ema
"""

# check correlations with confirmed_ema
heatmap_numeric_w_dependent_variable(covid_df_corr, 'confirmed_ema')

"""We find the following feature variables display the strong and mediumm correlation with the number of _confirmed_ema_

- Lag_confirmed_6
- Lag_confirmed_4
- recovered_ema
- Lag_confirmed_1
- Lag_deaths_5
- Lag_deaths_3
- Lag_deaths_2
- deaths (*)

# COVID-19 Containment Measure Features

Now we are going to load the features related to the timeline with the national containment measures, as maintained by the Oxford reasearch team per https://www.kaggle.com/davidoj/covid19-national-responses-dataset, with features defined in https://www.notion.so/Tag-hierarchy-Features-db9799312efa4f88851e8d49393bbb16
"""

# Load the log of COVID-19 Containment measure features
covid_containment_features_path = base_dataset_folder + '/covid19-national-responses-dataset/countermeasures_db_johnshopkins_2020_03_30.csv'

covid_containment_feature_raw = pd.read_csv(covid_containment_features_path)

# pre-processing

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['Unnamed: 0', 'Confirmed Cases', 'Deaths']),
    pdp.ColRename({
        'Date': 'covid_date',
        'Country': 'country_id'
        }),
    pdp.ApplyByCols(['covid_date'], pd.to_datetime),
])

covid_containment_feature_df = pipeline.apply(covid_containment_feature_raw)

covid_containment_feature_df = covid_containment_feature_df.fillna(0)

display(covid_containment_feature_df)

"""Below is the list on containment features provided"""

cols = covid_containment_feature_df.columns.values.tolist()

display(cols)

"""*Note:* You can refer to https://www.notion.so/Tag-hierarchy-Features-db9799312efa4f88851e8d49393bbb16 for the definition of each feature above."""

# read subset of features for Germany only
fcg_countries = {
    'Germany': 'DE'
}

pipeline = pdp.PdPipeline([
    pdp.ValKeep(fcg_countries.keys(), columns=['country_id']),
    pdp.MapColVals(['country_id'], fcg_countries)

])

covid_containment_de_df = pipeline.apply(covid_containment_feature_df)

display(covid_containment_de_df)

display(covid_df_corr.columns.values)

"""Now we are merging the containment measure features with the COVID-19 pandemic features as well as costructing a dataframe with relevant features for correlation analysis"""

#join
covid_data_with_containment = covid_df_corr.merge(covid_containment_feature_df, on=['country_id', 'covid_date'], how='left')

# do the subset for correlation analysis
pipeline = pdp.PdPipeline([
    pdp.ColDrop([
      'country_id', 'covid_date', 
      'Lag_confirmed_2', 'Lag_confirmed_3',
      'Lag_confirmed_4', 'Lag_confirmed_5', 'Lag_confirmed_1',
      'Trend_confirmed_2', 'Trend_confirmed_3',
      'Trend_confirmed_4', 'Trend_confirmed_5', 'Trend_confirmed_1',
      'Lag_deaths_2', 'Lag_deaths_3',
      'Lag_deaths_4', 'Lag_deaths_5', 'Lag_deaths_1',
      'Trend_deaths_2', 'Trend_deaths_3',
      'Trend_deaths_4', 'Trend_deaths_5', 'Trend_deaths_1',
      'Lag_recovered_2', 'Lag_recovered_3',
      'Lag_recovered_4', 'Lag_recovered_5', 'Lag_recovered_1',
      'Trend_recovered_2', 'Trend_recovered_3',
      'Trend_recovered_4', 'Trend_recovered_5', 'Trend_recovered_1',
      'day_num'])
])

covid_data_with_containment_corr = pipeline.apply(covid_data_with_containment)

"""## Summary of Findings

We find the following containment measure-related features to show strong correlation with *recovered_ema*

- Testing criteria
- Testing (number of tested people due date)
- Diagnostics criteria tightened
- Domestic travel restriction (*)
- Resumption (*)

At the same time, we could not detect any strong correlation of any of the containment measure features with either *confirmed*, *confirmed_ema*, or *deaths_ema*.

*Notes:*

- sparcity of the data about containment measures as well as relatively small number of observations (dates) with the quality containment measures in place (due to majority of nations hitting the pandemy in mid Mar 2020 only) made it a little tricky to apply it for a classical correlation analysis
- the set of containment measure feautes is quite good for simulation modelling as is though

## Detailed Correlation Analysis for Containment Measures

### Correlation with confirmed cases
"""

# check correlations with Confirmed
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'confirmed')

"""We can see there is no any strong correlation observed between *confirmed* and any feature derived from containment activities

### Correlation with Trend_confirmed_6
"""

# check correlations with Trend_confirmed_1
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'Trend_confirmed_6')

"""We can see there is no any strong correlation observed between *Trend_confirmed_6* and any feature derived from containment activities

### Correlation with Lag_confirmed_6
"""

# check correlations with Lag_confirmed_1
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'Lag_confirmed_6')

"""We can see there is no any strong correlation observed between *Lag_confirmed_6* and any feature derived from containment activities

### Correlation with Exponensial Moving Average for Confirmed Cases
"""

# check correlations with confirmed_ema
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'confirmed_ema')

"""We find that there is no strong correlatio displayed between the containment measure features and *confirmed_ema*

### Correlation with Trend_deaths_6
"""

# check correlations with Trend_deaths_1
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'Trend_deaths_6')

"""We find no good correlations between containment measure features and *Trend_deaths_6*

### Correlation with Lag_deaths_6
"""

# check correlations with Lag_deaths_6
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'Lag_deaths_6')

"""We find no good correlations between containment measure features and *Lag_deaths_6*

### Correlation With Exponensial Moving Average for Fatal Cases
"""

# check correlations with deaths_ema
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'deaths_ema')

"""We find that there is no strong correlation displayed between the containment measure features and *deaths_ema*

### Correlations with Trend_recovered_6
"""

# check correlations with Trend_recovered_1
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'Trend_recovered_6')

"""We find that there is no strong correlation displayed between the containment measure features and *Trend_recovered_6*

### Correlation with Lag_recovered_6
"""

# check correlations with Lag_recovered_6
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'Lag_recovered_6')

"""We find that there is no strong correlation displayed between the containment measure features and *Lag_recovered_6*

### Correlation With Exponensial Moving Average for Recovered Cases
"""

# check correlations with recovered_ema
heatmap_numeric_w_dependent_variable(covid_data_with_containment_corr, 'recovered_ema')

"""We find the following containment meature-related features to show strong correlation with *recovered_ema*

- Testing criteria
- Testing (number of tested people due date)
- Diagnostics criteria tightened
- Domestic travel restriction (*)
- Resumption (*)

# COVID-19 and National Economies / Healthcare / Demographics / Cultural Attitudes


Сorrelation of COVID-19 pandemic metrics (confirmed, deaths, recovered) with various parameters on  National Economies (macro-economic parameters) / Healthcare / Demographics / Cultural Attitudes will be investigated in this section. The major purpose of the research is to find candidate parameters to cluster the countries by.

Then it will be possible to better predict COVID-19 spread on different country clusters in cluster-then-predict fashion.

## Synopsys of Findings

It has been discovered that clustering countries by parameters below could add some edge

- female obesity index combined with female overweight index
- level of death due to outdoor air pollution

The atttbitutes in Economic Freedom Index, 2019 demonstrated strong correlation with *recovered_rate* and *recovered_ema*
- *fdi_inflow (FDI Inflow (Millions))* - strong correlation with *recovered_rate* and *recovered_ema*
- *GDP in 2019* (very strong correlation)
- *gdp_5_growth_rate* - Average rate of GDP growth across the last 5 years, strong correlation with *recovered_rate* and *recovered_ema*
- *gdp_tax_burden* - moderate-to-strong correlation between the gdp_tax_burden and COVID-19 spread attributes
- *income_tax* - moderate-to-strong correlation with *recovery_rate*
- *investom_freedom* - moderate-to-strong negative correlation with the *recovery_rate*
- *judical_effectiveness* - moderate-to-strong correlation between *judical_effectiveness* and *recovery_rate*
- *pub_debt* - medium correlation with *deaths_ema*

The inex of happiness 2020 data did not draw strong correlations. However, we still observed some effects as follows

- medium correlation between *'Perceptions of corruption'* and *recovery_rate*
- *'Explained by: Log GDP per capita'* to have a strong-to-medium correlation with *recovery_rate* as well as medium correlation with *confirmed_ema* and *deaths_ema*
- *'Explained by: Social support'* to have medium correlation with *recovery_rate* and *deaths_ema*
- *'Explained by: Healthy life expectancy'* to ahve strong correlation with *recovery_rate* as well as medium correlation with *deaths_ema* and *confirmed_ema*
- *'Explained by: Freedom to make life choices'* to have strong correlation with *recovery_rate* as well as medium negative correlation with *mortality_rate*
- *'Explained by: Perceptions of corruption'* to have medium correlation with *recovery_rate*
- *'Dystopia + residual'* to have medium correlation with *mortality_rate* as well as medium negative correlation with *recovery_rate*

You can refer to subsections below to see more details.

## COVID-19 and Population Forecast Data 2020 from Worldometers

Load Population Forecast Data 2020 from Worldometers:
"""

population_2020_path = base_dataset_folder + '/population-by-country-2020/population_by_country_2020.csv'

# Load countries data file
world_population_raw = pd.read_csv(population_2020_path)

world_population_raw.head(5)

# Prepare country name transformation
country_names = world_population_raw['Country (or dependency)'].unique().tolist()

# Create a zip object from two lists
zipbObj = zip(country_names, country_names)
 
# Create a dictionary from zip object
dictOfCountries = dict(zipbObj)

# overwrite a mapping for 'United States'
dictOfCountries['United States'] = 'US'
# Kosovo and Cruise Ship manually
dictOfCountries['Kosovo'] = 'Kosovo'
dictOfCountries['Diamond Princess'] = 'Diamond Princess'

# Add Kosovo and Cruise Ship data manually
# https://en.wikipedia.org/wiki/Demographics_of_Kosovo
# https://www.indexmundi.com/kosovo/#Demographics
# https://en.wikipedia.org/wiki/Diamond_Princess_(ship)
df1 = pd.DataFrame({'Country (or dependency)':['Kosovo','Diamond Princess'],
                        'Population (2020)':[1793000,3711],
                        'Yearly Change':[0.64,0],
                        'Net Change':[1147,0],
                        'Density (P/Km²)':[165,26],
                        'Land Area (Km²)':[10887,141],
                        'Migrants (net)':[-7340,0],
                        'Fert. Rate':[2.09,0],
                        'Med. Age':[30,62],
                        'Urban Pop %':['65%','100%'],
                        'World Share':[0.02,0.00]})
    
world_population_raw = world_population_raw.append(df1)

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['Fert. Rate', 'Migrants (net)', 'Yearly Change', 'World Share']),
    pdp.ColRename({'Country (or dependency)': 'country_id', 
                   'Population (2020)': 'pop_2020', 
                   'Net Change': 'net_growth_from_2019', 
                   'Density (P/Km²)': 'pop_density',
                   'Land Area (Km²)': 'land_area',
                   'Med. Age': 'med_age',
                   'Urban Pop %': 'urban_pop_ratio'
                   }),
    pdp.MapColVals(['country_id'], dictOfCountries)
])

world_population = pipeline.apply(world_population_raw)

# Remove the % character from Urban Pop values
world_population['urban_pop_ratio'] = world_population['urban_pop_ratio'].str.rstrip('%')

# Replace Urban Pop and Med Age "N.A" by their respective modes, then transform to int
world_population.loc[world_population['urban_pop_ratio']=='N.A.', 'urban_pop_ratio'] = int(world_population.loc[world_population['urban_pop_ratio']!='N.A.', 'urban_pop_ratio'].mode()[0])
world_population['urban_pop_ratio'] = world_population['urban_pop_ratio'].astype('int16')
world_population.loc[world_population['med_age']=='N.A.', 'med_age'] = int(world_population.loc[world_population['med_age']!='N.A.', 'med_age'].mode()[0])
world_population['med_age'] = world_population['med_age'].astype('int16')

print("Cleaned country details dataset")
display(world_population)

"""Now we will merge world_population and the slice of covid_df for the latest date as well as calculate essential relative metrics"""

pipeline = pdp.PdPipeline([
    pdp.ValKeep([covid_end_date], columns = ['covid_date'])              
])
# countries_without_china, columns=['country_id']
covid_latest_date_df = pipeline.apply(covid_df_corr)

#join
all_demo_data = covid_latest_date_df.merge(world_population, on='country_id', how='left')

pipe_calc = pdp.PdPipeline([
    pdp.ApplyToRows(lambda row: (row['confirmed']/row['pop_2020']), 'confirmed2pop'),
    pdp.ApplyToRows(lambda row: (row['confirmed_ema']/row['pop_2020']), 'confirmed_ema2pop'),
    pdp.ApplyToRows(lambda row: (row['net_growth_from_2019']/row['pop_2020']), 'pop_growth'),                 
])

# calculate ratio attributes
all_demo_data = pipe_calc.apply(all_demo_data)

display(all_demo_data)

display(all_demo_data.columns.values.tolist())

"""Now we will prepare subset of metrics for correlation analysis"""

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['country_id',  'covid_date',
                 'confirmed',	'deaths',	'recovered',	'pop_2020',	
                 'net_growth_from_2019', 'land_area',
                 'day_num'])
])

all_demo_data_corr = pipeline.apply(all_demo_data)

corr = all_demo_data_corr.corr()

ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation of  COVID-19 Ratios with Key Demographic Indicators')
ax

display(corr)

"""The correlations with _*confirmed_ema*_ will look as follows"""

# check correlations with confirmed_ema2pop
heatmap_numeric_w_dependent_variable(all_demo_data_corr, 'confirmed_ema2pop')

"""The key findings are as follows

* There is no strong correlation between % of confirmed COVID-19 cases among the national population (as well as the ratio of confirmed_ema to the total national population) and the number of confirmed cases

The correlation with _pop_growth_ is as follows
"""

# check correlations with pop_growth
heatmap_numeric_w_dependent_variable(all_demo_data_corr, 'pop_growth')

"""The key findings are as follows

* There is no strong correlation between % of confirmed COVID-19 cases among the national population (as well as the ratio of confirmed_ema to the total national population) and the population growth ratio vs. the past year

## COVID-19 and WHO Obesity Index

Load WHO Obesity Index data
"""

who_obesity_2016_path = base_dataset_folder + '/who-obesity-by-country-2016/WHO_obesityByCountry_2016.csv'

# Load countries data file
who_obesity_raw = pd.read_csv(who_obesity_2016_path)

who_obesity_raw.head(5)

"""Now we will transform the raw obesity dataframe to prepare it to merge with  COVID-19 pandemic metrics"""

# Prepare country name transformation
country_names = who_obesity_raw['Unnamed: 0'].unique().tolist()

# Create a zip object from two lists
zipbObj = zip(country_names, country_names)
 
# Create a dictionary from zip object
dictOfCountries = dict(zipbObj)

# overwrite a mapping for the countries with different names in WHO Obesity and COVID-19 datasets
dictOfCountries['United States of America'] = 'US'
dictOfCountries['Côte d\'Ivoire'] = 'Cote d\'Ivoire'
dictOfCountries['United Kingdom of Great Britain and Northern Ireland'] = 'United Kingdom'
dictOfCountries['Republic of North Macedonia'] = 'North Macedonia'
dictOfCountries['Taiwan'] = 'Taiwan*'
dictOfCountries['Republic of Korea'] = 'Korea, South'
dictOfCountries['Congo'] = 'Congo (Brazzaville)'
dictOfCountries['Democratic Republic of the Congo'] = 'Congo (Kinshasa)'
dictOfCountries['United Republic of Tanzania'] = 'Tanzania'
dictOfCountries['Viet Nam'] = 'Vietnam'
dictOfCountries['Republic of Moldova'] = 'Moldova'
dictOfCountries['Iran (Islamic Republic of)'] = 'Iran'
dictOfCountries['Brunei Darussalam'] = 'Brunei'
dictOfCountries['Russian Federation'] = 'Russia'
dictOfCountries['Venezuela (Bolivarian Republic of)'] = 'Venezuela'
dictOfCountries['Bolivia (Plurinational State of)'] = 'Bolivia'
dictOfCountries['Lao People\'s Democratic Republic'] = 'Laos'
dictOfCountries['Syrian Arab Republic'] = 'Syria'
dictOfCountries['Liechtenstein'] = 'Liechtenstein'
 

pipeline = pdp.PdPipeline([
    pdp.ColRename({'Unnamed: 0': 'country_id', 
                   'Both.sexes': 'combined_obesity', 
                   'Male': 'male_obesity', 
                   'Female': 'female_obesity'}),
    pdp.MapColVals(['country_id'], dictOfCountries)
])

who_obesity_df = pipeline.apply(who_obesity_raw)

display(who_obesity_df)

"""Now we are going to check the distributions of the obesity features"""

def histograms_numeric_columns(df, numerical_columns):
    '''
    Takes df, numerical columns as list
    Returns a group of histagrams
    '''
    f = pd.melt(df, value_vars=numerical_columns) 
    g = sns.FacetGrid(f, col='variable',  col_wrap=4, sharex=False, sharey=False)
    g = g.map(sns.distplot, 'value')
    return g

obesity_numerical_columns = ['combined_obesity',	'male_obesity',	'female_obesity']

histograms_numeric_columns(who_obesity_df, obesity_numerical_columns)

"""Now we will merge *covid_latest_date_df* and the pre-processed/transformed WHO obesity dataframe"""

#join
all_obesity_data = covid_latest_date_df.merge(who_obesity_df, on='country_id', how='left')

pipe_calc = pdp.PdPipeline([
    pdp.ApplyToRows(lambda row: (row['deaths']/row['confirmed']), 'mortality_rate'),
    pdp.ApplyToRows(lambda row: (row['recovered']/row['confirmed']), 'recovery_rate'),                
])

# calculate ratio attributes
all_obesity_data = pipe_calc.apply(all_obesity_data)


all_obesity_data.head(5)

"""Now we will prepare subset of metrics for correlation analysis"""

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['country_id', 'covid_date',
                 'confirmed',	'deaths',	'recovered', 
                 'day_num'])
])

all_obesity_data_corr = pipeline.apply(all_obesity_data)

corr = all_obesity_data_corr.corr()

ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation of COVID-19 Ratios with WHO Obesity Indicies')
ax

display(corr)

"""Now we will take a closer look at the correlations with _combined_obesity_"""

# check correlations with combined_obesity
heatmap_numeric_w_dependent_variable(all_obesity_data_corr, 'combined_obesity')

"""Now we will take a closer look at the correlations with _female_obesity_"""

# check correlations with female_obesity
heatmap_numeric_w_dependent_variable(all_obesity_data_corr, 'female_obesity')

"""Now we will take a closer look at the correlations with _male_obesity_"""

# check correlations with male_obesity
heatmap_numeric_w_dependent_variable(all_obesity_data_corr, 'male_obesity')

"""We find that 

* mortality rate  is not correlated with any of the obesity metrics
* recovery rare has quite strong negative correlation with *female_obesity* as well as moderate negative correlation with *combined_obesity*) 
* as a result, we may find it benefitial to cluster countries within the space of *female_obesity* and *combined_obesity* dimensions, and then use the resulted clusters in building the prediction models for recovered cases in the cluster-then-predict fashion

## COVID-19 and WHO Overweight Index

Load WHO Overweight Index data
"""

who_overweight_2016_path = base_dataset_folder + '/who-overweight-by-country-2016/WHO_overweightByCountry_2016.csv'

# Load WHO overweight data file
who_overweight_raw = pd.read_csv(who_overweight_2016_path)

who_overweight_raw.head(5)

"""Now we will transform the raw overweight dataframe to prepare it to merge with  COVID-19 pandemic metrics"""

# Prepare country name transformation
country_names = who_overweight_raw['Unnamed: 0'].unique().tolist()

# Create a zip object from two lists
zipbObj = zip(country_names, country_names)
 
# Create a dictionary from zip object
dictOfCountries = dict(zipbObj)

# overwrite a mapping for the countries with different names in WHO Obesity and COVID-19 datasets
dictOfCountries['United States of America'] = 'US'
dictOfCountries['Côte d\'Ivoire'] = 'Cote d\'Ivoire'
dictOfCountries['United Kingdom of Great Britain and Northern Ireland'] = 'United Kingdom'
dictOfCountries['Republic of North Macedonia'] = 'North Macedonia'
dictOfCountries['Taiwan'] = 'Taiwan*'
dictOfCountries['Republic of Korea'] = 'Korea, South'
dictOfCountries['Congo'] = 'Congo (Brazzaville)'
dictOfCountries['Democratic Republic of the Congo'] = 'Congo (Kinshasa)'
dictOfCountries['United Republic of Tanzania'] = 'Tanzania'
dictOfCountries['Viet Nam'] = 'Vietnam'
dictOfCountries['Republic of Moldova'] = 'Moldova'
dictOfCountries['Iran (Islamic Republic of)'] = 'Iran'
dictOfCountries['Brunei Darussalam'] = 'Brunei'
dictOfCountries['Russian Federation'] = 'Russia'
dictOfCountries['Venezuela (Bolivarian Republic of)'] = 'Venezuela'
dictOfCountries['Bolivia (Plurinational State of)'] = 'Bolivia'
dictOfCountries['Lao People\'s Democratic Republic'] = 'Laos'
dictOfCountries['Syrian Arab Republic'] = 'Syria'
dictOfCountries['Liechtenstein'] = 'Liechtenstein'
 

pipeline = pdp.PdPipeline([
    pdp.ColRename({'Unnamed: 0': 'country_id', 
                   'Both.sexes': 'combined_overweight', 
                   'Male': 'male_overweight', 
                   'Female': 'female_overweight'}),
    pdp.MapColVals(['country_id'], dictOfCountries)
])

who_overweight_df = pipeline.apply(who_overweight_raw)

display(who_overweight_df)

"""Now we will take a lot at  the distribution of the overweight-related numeric features"""

overweight_numerical_columns = ['combined_overweight', 'male_overweight',	'female_overweight']

histograms_numeric_columns(who_overweight_df, overweight_numerical_columns)

"""Now we will merge *covid_latest_date_df* and the pre-processed/transformed WHO overweight dataframe"""

#join
all_overweight_data = covid_latest_date_df.merge(who_overweight_df, on='country_id', how='left')

pipe_calc = pdp.PdPipeline([
    pdp.ApplyToRows(lambda row: (row['deaths']/row['confirmed']), 'mortality_rate'),
    pdp.ApplyToRows(lambda row: (row['recovered']/row['confirmed']), 'recovery_rate'),                
])

# calculate ratio attributes
all_overweight_data = pipe_calc.apply(all_overweight_data)


all_overweight_data.head(5)

"""Now we will prepare subset of metrics for correlation analysis"""

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['country_id', 'covid_date',
                 'confirmed',	'deaths',	'recovered',
                 'day_num'])
])

all_overweight_data_corr = pipeline.apply(all_overweight_data)

corr = all_overweight_data_corr.corr()

ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation of COVID-19 Ratios with WHO Obesity Indicies')
ax

display(corr)

"""Now we will take a closer look at the correlations with _combined_overweight_"""

# check correlations with combined_overweight
heatmap_numeric_w_dependent_variable(all_overweight_data_corr, 'combined_overweight')

"""Now we will take a closer look at the correlations with _male_overweight_"""

# check correlations with male_overweight
heatmap_numeric_w_dependent_variable(all_overweight_data_corr, 'male_overweight')

"""Now we will take a closer look at the correlations with _female_overweight_"""

# check correlations with female_overweight
heatmap_numeric_w_dependent_variable(all_overweight_data_corr, 'female_overweight')

"""We find that 

* mortality rate  is not correlated with any of the overweight metrics
* recovery rare has relatively strong negative correlation with *female_overweight* 
* as a result, we may want to cluster countries by *female_overweight* (or event combine it with obesity-related measures), and then use the obtained clustering in forecasting recovered cases in the cluster-then-predict fashion

## COVID and Air Pollution Metrics

We will load air pollution data set first
"""

air_pollution_data_path = base_dataset_folder + '/pollution-by-country-for-covid19-analysis/region_pollution.csv'

# Load countries data file
air_pollution_raw = pd.read_csv(air_pollution_data_path)

air_pollution_raw.head(5)

"""Now we will transform the raw data (with the metrics of death rates due to indoor and outdoor pollution) into the format ready to merge with COVID-19 pandemic metrics"""

# Prepare country name transformation
country_names = air_pollution_raw['Region'].unique().tolist()

# Create a zip object from two lists
zipbObj = zip(country_names, country_names)
 
# Create a dictionary from zip object
dictOfCountries = dict(zipbObj)

# overwrite a mapping for the countries with different names in WHO Obesity and COVID-19 datasets
dictOfCountries['United States of America'] = 'US'

pipeline = pdp.PdPipeline([
    pdp.ColRename({'Region': 'country_id', 
                   'Outdoor Pollution (deaths per 100000)': 'outdoor_poll_death_rate', 
                   'Indoor Pollution (deaths per 100000)': 'indoor_poll_death_rate'}),
    pdp.MapColVals(['country_id'], dictOfCountries)
])

air_pollution_df = pipeline.apply(air_pollution_raw)

display(air_pollution_df)

"""Now we will be looking at the distribution of the numeric attributes related to air polution"""

air_pollution_numerical_columns = ['outdoor_poll_death_rate',	'indoor_poll_death_rate']

histograms_numeric_columns(air_pollution_df, air_pollution_numerical_columns)

"""Now we will merge *covid_latest_date_df* and the pre-processed/transformed pollution-driven death rates dataframe"""

#join
all_air_pollution_data = covid_latest_date_df.merge(air_pollution_df, on='country_id', how='left')

pipe_calc = pdp.PdPipeline([
    pdp.ApplyToRows(lambda row: (row['deaths']/row['confirmed']), 'mortality_rate'),
    pdp.ApplyToRows(lambda row: (row['recovered']/row['confirmed']), 'recovery_rate'),                
])

# calculate ratio attributes
all_air_pollution_data = pipe_calc.apply(all_air_pollution_data)


all_air_pollution_data.head(5)

"""Now we will prepare the subset of data for correlation analysis"""

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['country_id', 'covid_date',
                 'confirmed',	'deaths',	'recovered',
                 'day_num'])
])

all_air_pollution_data_corr = pipeline.apply(all_air_pollution_data)

corr = all_air_pollution_data_corr.corr()

ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation of COVID-19 Ratios with WHO Obesity Indicies')
ax

display(corr)

"""Now we are going to take a closer look on the correlation with _outdoor_poll_death_rate_"""

# check correlations with outdoor_poll_death_rate
heatmap_numeric_w_dependent_variable(all_air_pollution_data_corr, 'outdoor_poll_death_rate')

"""Now we are going to take a closer look on the correlation with _indoor_poll_death_rate_"""

# check correlations with indoor_poll_death_rate
heatmap_numeric_w_dependent_variable(all_air_pollution_data_corr, 'indoor_poll_death_rate')

"""We find a no strong or even medium correlations between COVID-19 pandemic data and *indoor_poll_death_rate* and *outdoor_poll_death_rate*.

## COVID-19 and Macro-Economic Parameters of Countries (Economic Freedom Index, 2019)

Reading the dataset with macro-economic parameters in memory
"""

eco_freedom_data_path = base_dataset_folder + '/the-economic-freedom-index/economic_freedom_index2019_data.csv'

# Load Economic Freedom Index 2019 data file
eco_raw = pd.read_csv(eco_freedom_data_path, encoding= "ISO-8859-1")

eco_raw.head(5)

"""Now we will transform the raw Economic Freedom data into the format ready to merge with COVID-19 pandemic metrics"""

# Prepare country name transformation
country_names = eco_raw['Country'].unique().tolist()

# Create a zip object from two lists
zipbObj = zip(country_names, country_names)
 
# Create a dictionary from zip object
dictOfCountries = dict(zipbObj)

# overwrite a mapping for the countries with different names in WHO Obesity and COVID-19 datasets
dictOfCountries['United States of America'] = 'US'
dictOfCountries['Côte d\'Ivoire'] = 'Cote d\'Ivoire'
dictOfCountries['Republic of North Macedonia'] = 'North Macedonia'
dictOfCountries['Taiwan '] = 'Taiwan*'
dictOfCountries['Congo, Republic of'] = 'Congo (Brazzaville)'
dictOfCountries['Congo, Democratic Republic of the Congo'] = 'Congo (Kinshasa)'
dictOfCountries['Slovak Republic'] = 'Slovakia'
dictOfCountries['Kyrgyz Republic'] = 'Kyrgyzstan'
dictOfCountries['Brunei Darussalam'] = 'Brunei'
dictOfCountries['Macedonia'] = 'North Macedonia'
dictOfCountries['Lao P.D.R.'] = 'Laos'

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['CountryID', 'Country Name', 'WEBNAME', 'Population (Millions)']),
    pdp.ColRename({'Country': 'country_id', 
                   'Region': 'region',
                   'World Rank': 'world_rank', 
                   'Region Rank': 'region_rank',
                   '2019 Score': 'score_2019',
                   'Property Rights': 'property_rights',	
                   'Judical Effectiveness': 'judical_effectiveness',
                   'Government Integrity': 'gov_integrity',
                   'Tax Burden': 'tax_burden',
                   'Gov\'t Spending': 'gov_spending',
                   'Fiscal Health': 'fiscal_hlth',
                   'Business Freedom': 'business_freedom',
                   'Labor Freedom': 'labor_freedom',
                   'Monetary Freedom': 'monetary_freedom',
                   'Trade Freedom': 'trade_freedom',
                   'Investment Freedom ': 'investment_freedom',
                   'Financial Freedom': 'fin_freedom',
                   'Tariff Rate (%)': 'tariff_rate',
                   'Income Tax Rate (%)': 'income_tax',
                   'Corporate Tax Rate (%)': 'corporate_tax',
                   'Tax Burden % of GDP': 'gdp_tax_burden',
                   'Gov\'t Expenditure % of GDP ': 'gdp_gov_exp_rate',
                   'GDP (Billions, PPP)': 'gdp',
                   'GDP Growth Rate (%)': 'gdp_growth_rate',
                   '5 Year GDP Growth Rate (%)': 'gdp_5_growth_rate',
                   'GDP per Capita (PPP)': 'gdp_per_capita',
                   'Unemployment (%)': 'unemp_rate',
                   'Inflation (%)': 'infl_rate',
                   'FDI Inflow (Millions)': 'fdi_inflow',
                   'Public Debt (% of GDP)': 'pub_debt'
                   }),
    pdp.MapColVals(['country_id'], dictOfCountries)
])

eco_df = pipeline.apply(eco_raw)

# add countries that are missing in the Eco Freedom dataset yet present in
# COVID-19 pandemic data

df1 = pd.DataFrame(
    {'country_id':[
      'Guernsey','Andorra','Greenland','Aruba','Diamond Princess','San Marino',
      'Jersey','Antigua and Barbuda','French Guiana',
      'Puerto Rico','Mayotte','Holy See','Reunion','Guam','Martinique','Guadeloupe','Monaco','Czechia', 'Saint Kitts and Nevis', 'Grenada'],   
                        
      'region':[
          'Europe','Europe','Americas','Americas','Asia-Pacific','Europe','Europe',
          'Americas','Americas',
          'Americas','Sub-Saharan Africa','Europe','Sub-Saharan Africa','Asia-Pacific',
          'Americas','Americas','Europe','Europe', 'Americas', 'Americas']})
    
eco_df = eco_df.append(df1, sort=True)

# cast data types on certain variables
eco_df['gdp'] = eco_df['gdp'].str.strip('$').str.split(' ').str.get(0).str.replace(',', '').astype(float)
eco_df['gdp_per_capita'] = eco_df['gdp_per_capita'].str.strip('$').str.split(' ').str.get(0).str.replace(',', '').astype(float)
eco_df['unemp_rate'] = eco_df['unemp_rate'].str.split(' ').str.get(0).astype(float)
eco_df['fdi_inflow'] = eco_df['fdi_inflow'].str.replace(',', '').astype(float)

# impute NAs: fill missing values with region medians
for col in eco_df.columns[eco_df.isna().any()].tolist():
  eco_df[col] = eco_df[col].fillna(eco_df.groupby('region')[col].transform('median')) 

display(eco_df)

"""Now we will investigate relations between Economic Freedom Index variables before we merge it with COVID-19 pandemic features"""

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['region', 'country_id'])
])

eco_corr_df = pipeline.apply(eco_df)

# sns.pairplot(eco_corr_df)

corr = eco_corr_df.corr()

ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation of COVID-19 Ratios with WHO Obesity Indicies')
ax

"""Now let's merge the economic freedom parameters with the latest COVID-19 epidemic metrics"""

#join
all_eco_data = covid_latest_date_df.merge(eco_df, on='country_id', how='left')

pipe_calc = pdp.PdPipeline([
    pdp.ApplyToRows(lambda row: (row['deaths']/row['confirmed']), 'mortality_rate'),
    pdp.ApplyToRows(lambda row: (row['recovered']/row['confirmed']), 'recovery_rate'), 
    pdp.ColDrop(['region'])               
])

# calculate ratio attributes
all_eco_data = pipe_calc.apply(all_eco_data)

all_eco_data.head(5)

"""Now we will check the correlation of every attribute of Economic Freedom Index with COVID-19 pandemic parameters"""

def economic_freedom_param_correlations(covid_df, eco_df, target_col):
  col_list = eco_df.columns.values.tolist() 
  keep_list = ['country_id', target_col]

  drop_list = []
  for col in col_list:
    if col not in keep_list:
      new_list = [col]
      drop_list = drop_list + new_list

  pipe_filter_param = pdp.PdPipeline([
    pdp.ColDrop(drop_list)               
  ])

  eco_param_df = pipe_filter_param.apply(eco_df)

  #join
  eco_param_data = covid_df.merge(eco_param_df, on='country_id', how='left')

  pipeline = pdp.PdPipeline([
    pdp.ColDrop(['country_id', 'covid_date',
                 'confirmed',	'deaths',	'recovered',
                 'day_num'])
  ])

  eco_param_corr = pipeline.apply(eco_param_data)

  return heatmap_numeric_w_dependent_variable(eco_param_corr, target_col)

"""We will check correlations with _'business_freedom'_"""

economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'business_freedom')

"""We find no strong correlations between _'business_freedom'_ and COVID-19 epidemic metrics

Now we are going to look at the correlation between _corporate_tax_ and the COVID-19 pandemic metrics
"""

# check correlations with corporate_tax
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'corporate_tax')

"""We find no strong correlations between _'corporate_tax'_ and COVID-19 epidemic metrics

Now we are going to look at the correlation between _fdi_inflow_ and the COVID-19 pandemic metrics
"""

# check correlations with fdi_inflow
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'fdi_inflow')

"""We can see quite a strong correlation of _fdi_inflow_ (FDI Inflow (Millions)) with _recovery_rate_, and therefore fdi_inflow can be used in future clustering experiments.

Now we are going to look at the correlation between _fin_freedom_ and the COVID-19 pandemic metrics
"""

# check correlations with fin_freedom
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'fin_freedom')

"""We can see moderate negative correlation between the *financial freedom index* and *recovery_rate*

Now we are going to look at the correlation between _fiscal_hlth_ and the COVID-19 pandemic metrics
"""

# check correlations with fiscal_hlth
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'fiscal_hlth')

"""We can see no correlation between the *fisccal health* and any of the COVID-19 spread attributes.

Now we are going to look at the correlation between *gdp* and the COVID-19 pandemic metrics
"""

# check correlations with gdp
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gdp')

"""We can see quite a strong correlation of _gdp_ with _recovery_rate_ as well as the  moderate-to-strong correlation with _recovered_ema_, and therefore *gdp* can be used in future clustering experiments.

Now we are going to look at the correlation between *gdp_5_growth_rate* and the COVID-19 pandemic metrics
"""

# check correlations with gdp_5_growth_rate
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gdp_5_growth_rate')

"""We can see *gdp_5_growth_rate* to show relatively strong correlation with COVID-19 recovery rate.

Now we are going to look at the correlation between *gdp_gov_exp_rate* and the COVID-19 pandemic metrics
"""

# check correlations with gdp_gov_exp_rate
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gdp_gov_exp_rate')

"""We can see no any strong correlation of the Government expense rate (% of GDP) with COVID-19 spread attributes

Now we are going to look at the correlation between *gdp_growth_rate* and the COVID-19 pandemic metrics
"""

# check correlations with gdp_growth_rate
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gdp_growth_rate')

"""We see no any strong correlation of the GDP Growth Rate with COVID-19 spread attributes

Now we are going to look at the correlation between *gdp_per_capita* and the COVID-19 pandemic metrics
"""

# check correlations with gdp_per_capita
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gdp_per_capita')

"""We see no any strong correlation between the *GDP per Capita* rate and COVID-19 spread attributes

Now we are going to look at the correlation between *gdp_tax_burden* and the COVID-19 pandemic metrics
"""

# check correlations with gdp_tax_burden
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gdp_tax_burden')

"""We see relatively medium-to-strong correlation between the *gdp_tax_burden* and COVID-19 spread attributes

Now we are going to look at the correlation between *gov_integrity* and the COVID-19 pandemic metrics
"""

# check correlations with gov_integrity
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gov_integrity')

"""We see no strong correlation between *gov_integrity* and any of the COVID-19 pandemic metrics.

Now we are going to look at the correlation between *gov_spending* and the COVID-19 pandemic metrics
"""

# check correlations with gov_spending
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'gov_spending')

"""We see no strong correlation between *gov_spending* and any of the COVID-19 pandemic metrics.

Now we are going to look at the correlation between *income_tax* and the COVID-19 pandemic metrics
"""

# check correlations with income_tax
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'income_tax')

"""We find moderate-to-strong correlation between *income_tax* and *recovery_rate*

Now we are going to look at the correlation between *infl_rate (inflation rate)* and the COVID-19 pandemic metrics
"""

# check correlations with infl_rate
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'infl_rate')

"""We see no strong correlation between *infl_rate* and any of the COVID-19 pandemic metrics.

Now we are going to look at the correlation between *investment_freedom* and the COVID-19 pandemic metrics
"""

# check correlations with investment_freedom
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'investment_freedom')

"""We find *investom_freedom* to have moderate-to-strong negative correlation with the *recovery_rate*

Now we are going to look at the correlation between *judical_effectiveness* and the COVID-19 pandemic metrics
"""

# check correlations with judical_effectiveness
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'judical_effectiveness')

"""We find the moderate-to-strong correlation between *judical_effectiveness* and *recovery_rate*

Now we are going to look at the correlation between *labor_freedom* and the COVID-19 pandemic metrics
"""

# check correlations with labor_freedom
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'labor_freedom')

"""We see no strong correlations between *labor_freedom* and COVID-19 pandemic metrics

Now we are going to look at the correlation between *monetary_freedom* and the COVID-19 pandemic metrics
"""

# check correlations with monetary_freedom
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'monetary_freedom')

"""We find no strong correlations between *monetary_freedom* and the COVID-19 pandemic metrics

Now we are going to look at the correlation between *property_rights* and the COVID-19 pandemic metrics
"""

# check correlations with property_rights
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'property_rights')

"""We find no strong correlations between *property_rights* and the COVID-19 pandemic metrics

Now we are going to look at the correlation between *pub_debt* and the COVID-19 pandemic metrics
"""

# check correlations with pub_debt
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'pub_debt')

"""We can see *pub_debt* to have medium correlation with *deaths_ema*

Now we are going to look at the correlation between *region_rank* and the COVID-19 pandemic metrics
"""

# check correlations with region_rank
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'region_rank')

"""We see no strong correlation between *region_rank* and any of the COVID-19 pandemic metrics

Now we are going to look at the correlation between *score_2019* and the COVID-19 pandemic metrics
"""

# check correlations with score_2019
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'score_2019')

"""We see no strong correlation between *score_2019* and any of the COVID-19 pandemic metrics

Now we are going to look at the correlation between *tariff_rate* and the COVID-19 pandemic metrics
"""

# check correlations with tariff_rate
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'tariff_rate')

"""We see no strong correlation between *tariff_rate* and any of the COVID-19 pandemic metrics

Now we are going to look at the correlation between *tax_burden* and the COVID-19 pandemic metrics
"""

# check correlations with tax_burden
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'tax_burden')

"""We see no strong correlation between *tax_burden* and any of the COVID-19 pandemic metrics

Now we are going to look at the correlation between *trade_freedom* and the COVID-19 pandemic metrics
"""

# check correlations with trade_freedom
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'trade_freedom')

"""We see no strong correlation between *trade_freedom* and any of the COVID-19 pandemic metrics

Now we are going to look at the correlation between *unemp_rate* and the COVID-19 pandemic metrics
"""

# check correlations with unemp_rate
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'unemp_rate')

"""We see no strong correlation between *unemp_rate* and any of the COVID-19 pandemic metrics

Now we are going to look at the correlation between *world_rank* and the COVID-19 pandemic metrics
"""

# check correlations with world_rank
economic_freedom_param_correlations(covid_latest_date_df, eco_df, 'world_rank')

"""We see no strong correlation between *world_rank* and any of the COVID-19 pandemic metrics

## COVID-19 and Macro-Economic Parameters of Countries (World Happyness Report 2020)

First of all, we will load and transform the data for World Happiness report 2020
"""

happiness_data_path = base_dataset_folder + '/world-happiness-report-2020/WHR20_DataForFigure2.1.csv'

# Load World Happiness 2020 data file
happiness_raw = pd.read_csv(happiness_data_path, encoding= "ISO-8859-1")

happiness_raw.head(5)

"""Now we will transform the raw World Happiness data into the format ready to merge with COVID-19 pandemic metrics"""

# Prepare country name transformation
country_names = happiness_raw['Country name'].unique().tolist()

# Create a zip object from two lists
zipbObj = zip(country_names, country_names)
 
# Create a dictionary from zip object
dictOfCountries = dict(zipbObj)

# overwrite a mapping for the countries with different names in WHO Obesity and COVID-19 datasets
dictOfCountries['United States of America'] = 'US'
dictOfCountries['Ivory Coast'] = 'Cote d\'Ivoire'
dictOfCountries['Macedonia'] = 'North Macedonia'
dictOfCountries['Taiwan Province of China'] = 'Taiwan*'
dictOfCountries['South Korea'] = 'Korea, South'
dictOfCountries['Czech Republic'] = 'Czechia'
dictOfCountries['Swaziland'] = 'Eswatini'


pipeline = pdp.PdPipeline([
    pdp.ColDrop(['Regional indicator']),
    pdp.ColRename({'Country name': 'country_id'
                   }),
    pdp.MapColVals(['country_id'], dictOfCountries)
])

happiness_df = pipeline.apply(happiness_raw)

"""Now we will investigate relations between Word Happiness report variables before we merge it with COVID-19 pandemic features"""

pipeline = pdp.PdPipeline([
    pdp.ColDrop(['country_id'])
])

happiness_corr_df = pipeline.apply(happiness_df)

corr = happiness_corr_df.corr()

ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation of COVID-19 Ratios with WHO Obesity Indicies')
ax

"""Now let's merge the world happiness parameters with the latest COVID-19 epidemic metrics"""

#join
all_happiness_data = covid_latest_date_df.merge(happiness_df, on='country_id', how='left')

pipe_calc = pdp.PdPipeline([
    pdp.ApplyToRows(lambda row: (row['deaths']/row['confirmed']), 'mortality_rate'),
    pdp.ApplyToRows(lambda row: (row['recovered']/row['confirmed']), 'recovery_rate')          
])

# calculate ratio attributes
all_happiness_data = pipe_calc.apply(all_happiness_data)

all_happiness_data.head(5)

"""Now we will check the correlation of every attribute of World Happiness with COVID-19 pandemic parameters"""

def happiness_param_correlations(covid_df, happy_df, target_col):
  col_list = happy_df.columns.values.tolist() 
  keep_list = ['country_id', target_col]

  drop_list = []
  for col in col_list:
    if col not in keep_list:
      new_list = [col]
      drop_list = drop_list + new_list

  pipe_filter_param = pdp.PdPipeline([
    pdp.ColDrop(drop_list)               
  ])

  happy_param_df = pipe_filter_param.apply(happy_df)

  #join
  happy_param_data = covid_df.merge(happy_param_df, on='country_id', how='left')

  pipeline = pdp.PdPipeline([
    pdp.ColDrop(['country_id', 'covid_date',
                 'confirmed',	'deaths',	'recovered',
                 'day_num'])
  ])

  happy_param_corr = pipeline.apply(happy_param_data)

  return heatmap_numeric_w_dependent_variable(happy_param_corr, target_col)

"""We will check correlations with *'Ladder score'*"""

happiness_param_correlations(covid_latest_date_df, happiness_df, 'Ladder score')

"""We find moderate-to-weak correlations of *Ladder score* with *death_ema* and *confirmed_ema*

Now we will check the correlation wiht *'Standard error of ladder score'*
"""

happiness_param_correlations(covid_latest_date_df, happiness_df, 'Standard error of ladder score')

"""We can see its strong negative correllation *'Standard error of ladder score'* with *'recovery_rate'*

Now we will check the correlation wiht *'upperwhisker'*
"""

happiness_param_correlations(covid_latest_date_df, happiness_df, 'upperwhisker')

"""We find moderate-to-weak correlations of *upperwhisker* with *death_ema* and *confirmed_ema*

Now we will check the correlation wiht *'lowerwhisker'*
"""

happiness_param_correlations(covid_latest_date_df, happiness_df, 'lowerwhisker')

"""We find moderate-to-weak correlations of *lowerwhisker* with *death_ema* and *confirmed_ema*

Now we will check the correlation wiht *'Logged GDP per capita'*
"""

happiness_param_correlations(covid_latest_date_df, happiness_df, 'Logged GDP per capita')

"""We find moderate-to-weak correlations of *Logged GDP per capita* with *death_ema* and *confirmed_ema*

Now we will check the correlation wiht *'Social support'*
"""

happiness_param_correlations(covid_latest_date_df, happiness_df, 'Social support')

"""We find moderate-to-weak correlations of *'Social support'* with *death_ema* and *confirmed_ema*

Now we will check the correlation wiht *'Healthy life expectancy'*
"""

happiness_param_correlations(covid_latest_date_df, happiness_df, 'Healthy life expectancy')

"""We find moderate-to-weak correlations of *'Healthy life expectancy'* with *death_ema* and *confirmed_ema*

Now we will check the correlation wiht *'Freedom to make life choices'*
"""

#Freedom to make life choices
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Freedom to make life choices')

"""We find moderate-to-weak correlations of *'Freedom to make life choices'* with *death_ema* and *confirmed_ema*

Now we will check the correlation wiht *'Generosity'*
"""

# Generosity
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Generosity')

"""We find no strong correlation between *Generosity* and COVID-19 pandemic metrics

Now we will check the correlation wiht *'Perceptions of corruption'*
"""

# 'Perceptions of corruption'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Perceptions of corruption')

"""We see medium correlation between *'Perceptions of corruption'* and *recovery_rate*

Now we will check the correlation wiht *'Ladder score in Dystopia'*
"""

# 'Ladder score in Dystopia'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Ladder score in Dystopia')

"""We find no strong correlation between *'Ladder score in Dystopia'* and COVID-19 pandemic metrics

Now we will check the correlation wiht *'Explained by: Log GDP per capita'*
"""

# 'Ladder score in Dystopia'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Explained by: Log GDP per capita')

"""We see *'Explained by: Log GDP per capita'* to have a strong-to-medium correlation with *recovery_rate* as well as medium correlation with *confirmed_ema* and *deaths_ema*

Now we will check the correlation wiht *'Explained by: Social support'*
"""

# 'Explained by: Social support'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Explained by: Social support')

"""We see *'Explained by: Social support'* to have medium correlation with *recovery_rate* and *deaths_ema*

Now we will check the correlation wiht *'Explained by: Healthy life expectancy'*
"""

# 'Explained by: Healthy life expectancy'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Explained by: Healthy life expectancy')

"""We see *'Explained by: Healthy life expectancy'* to ahve strong correlation with *recovery_rate* as well as medium correlation with *deaths_ema* and *confirmed_ema*

Now we will check the correlation wiht *'Explained by: Freedom to make life choices'*
"""

# 'Explained by: Freedom to make life choices'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Explained by: Freedom to make life choices')

"""We can see *'Explained by: Freedom to make life choices'* to have strong correlation with *recovery_rate* as well as medium negative correlation with *mortality_rate*

Now we will check the correlation wiht *'Explained by: Generosity'*
"""

# 'Explained by: Generosity'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Explained by: Generosity')

"""We see *'Explained by: Generosity'* to have no any correlations with the COVID-19 pandemic features

Now we will check the correlation wiht *'Explained by: Perceptions of corruption'*
"""

# 'Explained by: Perceptions of corruption'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Explained by: Perceptions of corruption')

"""We can see *'Explained by: Perceptions of corruption'* to have medium correlation with *recovery_rate*

Now we will check the correlation wiht *'Dystopia + residual'*
"""

# 'Dystopia + residual'
happiness_param_correlations(covid_latest_date_df, happiness_df, 'Dystopia + residual')

"""We see *'Dystopia + residual'* to have medium correlation with *mortality_rate* as well as medium negative correlation with *recovery_rate*

# COVID-19 and Weather

This section will cover the analysis of correlation of COVID-19 pandemic metrics (confirmed, deaths, recovered) with weather parameters (to check the hypothesis of COVID-19 to be weather-sensitive, like influenza viruses)

TBD

# COVID-9 Forecast: Confirmed and Fatal Cases in the next 7 days

TBD

# Data Sources



1.   JHU CoronaVirus Dataset: https://github.com/CSSEGISandData/COVID-19
2.   COVID-19 containment and mitigation measures (https://www.kaggle.com/paultimothymooney/covid-19-containment-and-mitigation-measures/), using data from http://epidemicforecasting.org/containment
3.   Popolation forecasts 2020 (https://www.kaggle.com/tanuprabhu/population-by-country-2020, scrapped data from https://www.worldometers.info/world-population/population-by-country/, based on the latest United Nations Population Division estimates
4.   Additional datasource references (to be processed)
- https://medium.com/intuitive-machine-learning/covid-19-open-datasets-8432a7c085e0
"""